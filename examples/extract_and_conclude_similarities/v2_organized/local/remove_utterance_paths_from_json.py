#!/usr/bin/env python3
"""
ç§»é™¤æ‰€æœ‰JSONæ–‡ä»¶ä¸­çš„utterance_pathså­—æ®µï¼Œå‡å°‘æ–‡ä»¶å¤§å°
"""

import json
import argparse
from pathlib import Path
from tqdm import tqdm
import logging
import shutil
from concurrent.futures import ProcessPoolExecutor, as_completed

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def process_single_json(json_file, dry_run=False):
    """å¤„ç†å•ä¸ªJSONæ–‡ä»¶ï¼Œç§»é™¤utterance_pathså­—æ®µ"""
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«utterance_pathså­—æ®µ
        if 'utterance_paths' not in data:
            return {
                'file': str(json_file),
                'status': 'skipped',
                'reason': 'no utterance_paths field'
            }
        
        # è®¡ç®—åŸå§‹æ–‡ä»¶å¤§å°
        original_size = json_file.stat().st_size
        
        # ç§»é™¤utterance_pathså­—æ®µ
        utterance_paths_count = len(data.get('utterance_paths', []))
        del data['utterance_paths']
        
        if dry_run:
            # æ¨¡æ‹Ÿè®¡ç®—æ–°æ–‡ä»¶å¤§å°ï¼ˆä¼°ç®—ï¼‰
            new_data_str = json.dumps(data, indent=2, ensure_ascii=False)
            estimated_size = len(new_data_str.encode('utf-8'))
            size_reduction = original_size - estimated_size
            return {
                'file': str(json_file),
                'status': 'dry_run',
                'original_size': original_size,
                'estimated_size': estimated_size,
                'size_reduction': size_reduction,
                'utterance_paths_count': utterance_paths_count
            }
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_file = json_file.with_suffix('.json.tmp')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        
        # è·å–æ–°æ–‡ä»¶å¤§å°
        new_size = temp_file.stat().st_size
        
        # åŸå­æ€§æ›¿æ¢
        temp_file.replace(json_file)
        
        size_reduction = original_size - new_size
        
        return {
            'file': str(json_file),
            'status': 'success',
            'original_size': original_size,
            'new_size': new_size,
            'size_reduction': size_reduction,
            'utterance_paths_count': utterance_paths_count
        }
        
    except json.JSONDecodeError as e:
        return {
            'file': str(json_file),
            'status': 'error',
            'error': f'JSON decode error: {e}'
        }
    except Exception as e:
        return {
            'file': str(json_file),
            'status': 'error',
            'error': str(e)
        }

def find_json_files(base_dir):
    """æŸ¥æ‰¾æ‰€æœ‰utterance_similarities.jsonæ–‡ä»¶"""
    base_path = Path(base_dir)
    json_files = list(base_path.rglob('*_utterance_similarities.json'))
    return json_files

def main():
    parser = argparse.ArgumentParser(description='Remove utterance_paths field from JSON files')
    parser.add_argument('--base_dir', type=str, 
                        default='/root/group-shared/voiceprint/data/speech/speaker_diarization/merged_datasets_20250610_vad_segments_mtfaa_enhanced_extend_kid_withclone_addlibrilight_1130/embeddings_wespeaker_samresnet100/utterance_similarities_per_speaker',
                        help='Base directory containing JSON files')
    parser.add_argument('--num_workers', type=int, default=8,
                        help='Number of worker processes')
    parser.add_argument('--dry_run', action='store_true',
                        help='Dry run mode: only show what would be done without actually modifying files')
    
    args = parser.parse_args()
    logger = setup_logging()
    
    logger.info("=== Remove utterance_paths from JSON files ===")
    logger.info(f"Base directory: {args.base_dir}")
    logger.info(f"Number of workers: {args.num_workers}")
    logger.info(f"Dry run mode: {args.dry_run}")
    logger.info("=" * 50)
    
    # æŸ¥æ‰¾æ‰€æœ‰JSONæ–‡ä»¶
    logger.info("Scanning for JSON files...")
    json_files = find_json_files(args.base_dir)
    logger.info(f"Found {len(json_files)} JSON files")
    
    if not json_files:
        logger.warning("No JSON files found!")
        return
    
    # å¤„ç†æ–‡ä»¶
    total_processed = 0
    total_skipped = 0
    total_errors = 0
    total_size_reduction = 0
    total_original_size = 0
    total_new_size = 0
    
    error_messages = []
    
    with ProcessPoolExecutor(max_workers=args.num_workers) as executor:
        futures = {executor.submit(process_single_json, json_file, args.dry_run): json_file 
                   for json_file in json_files}
        
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing files"):
            json_file = futures[future]
            try:
                result = future.result()
                
                if result['status'] == 'success':
                    total_processed += 1
                    total_original_size += result['original_size']
                    total_new_size += result['new_size']
                    total_size_reduction += result['size_reduction']
                elif result['status'] == 'dry_run':
                    total_processed += 1
                    total_original_size += result['original_size']
                    total_new_size += result['estimated_size']
                    total_size_reduction += result['size_reduction']
                elif result['status'] == 'skipped':
                    total_skipped += 1
                elif result['status'] == 'error':
                    total_errors += 1
                    error_msg = result.get('error', 'Unknown error')
                    error_messages.append(f"{result['file']}: {error_msg}")
                    if len(error_messages) <= 20:
                        logger.warning(f"Error processing {result['file']}: {error_msg}")
                
            except Exception as e:
                total_errors += 1
                error_messages.append(f"{json_file}: {e}")
                logger.error(f"Error processing {json_file}: {e}", exc_info=True)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    logger.info("\n" + "=" * 50)
    logger.info("ğŸ“Š Processing Summary:")
    logger.info(f"  âœ… Processed: {total_processed} files")
    logger.info(f"  â­ï¸  Skipped: {total_skipped} files")
    logger.info(f"  âŒ Errors: {total_errors} files")
    
    if total_processed > 0:
        logger.info(f"\nğŸ’¾ Size Statistics:")
        logger.info(f"  Original total size: {total_original_size / (1024**2):.2f} MB")
        logger.info(f"  New total size: {total_new_size / (1024**2):.2f} MB")
        logger.info(f"  Total size reduction: {total_size_reduction / (1024**2):.2f} MB")
        logger.info(f"  Average size reduction per file: {total_size_reduction / total_processed / 1024:.2f} KB")
        if total_original_size > 0:
            reduction_percent = (total_size_reduction / total_original_size) * 100
            logger.info(f"  Size reduction percentage: {reduction_percent:.2f}%")
    
    if error_messages:
        logger.info(f"\nâš ï¸  Error Summary (showing up to 20 errors):")
        for msg in error_messages[:20]:
            logger.warning(f"  {msg}")
        if len(error_messages) > 20:
            logger.warning(f"  ... and {len(error_messages) - 20} more errors")
    
    if args.dry_run:
        logger.info("\nğŸ’¡ This was a dry run. Use without --dry_run to actually modify files.")
    else:
        logger.info("\nâœ… All files processed successfully!")

if __name__ == "__main__":
    main()

